## 4. Harms & Consequences

[Back to TOC](/?id=table-of-contents)

Societal consequences happen when we ignore or defer data ethics considerations. For instance, violation of (personal data) privacy or weaponization of (algorithm-driven) decisions can lead to potential harms at individual and collective group levels. Let's explore these in brief.

### 4.1 Personal Harms

Personal harms focus on damage done directly to individuals in the collection or use of their data. A typical context can be scientific research studies involving human participants. The [types of harm](https://research.virginia.edu/types-harm) possible are:

 * **Psychological** - where research subjects are put in uncomfortable or stressful situations.
 * **Physical** - where research subjects engage in actions that pose a risk of injury.
 * **Legal** - where subject's confidences may not be protected by doctor or lawyer confidentiality
 * **Social** - where inadvertently disclosing subjects' data could harm their standing in community
 * **Economic** - where subject participation incurs economic loss or undermines potential gains

### 4.2 Algorithmic Harms

Algorithmic harms focus on the potential damage done when we develop decision-making intelligence at scale. It can refer to both the curation and analysis of the data models (powering the algorithm) and to AI-driven design patterns or user experiences that implement these data-driven decisions.

This chart (credit: Megan Smith, former CTO of the United States) highlights individual and collective social harms that can occur due to algorithmic decision-making, highlighting the role of _bias (discrimination)_ and _unfairness (inequalities)_ that result in harms being indiscriminately worse for select groups of society.

![Potential](notes/megan-smith-algorithms.png) 

For a better understanding of types of AI bias (e.g., human bias, hidden bias, data sampling bias, long-tail bias and intentional bias) check out [this article](https://www.forbes.com/sites/glenngow/2020/11/09/how-ai-can-go-terribly-wrong-5-biases-that-create-failure/?sh=6f561b635b87) . For a better understanding of _Fairness-related harms_, check out [this lesson](https://github.com/microsoft/ML-For-Beginners/tree/main/1-Introduction/3-fairness#fairness-related-harms) from the **[ML For Beginners](https://github.com/microsoft/ML-For-Beginners)** curriculum. 


### 4.3 Systemic Harms

Even if we do solve issues like fairness, privacy and consent, we may still face  _systemic_ issues in data ethics because of the long history of data and entrenched norms that exist in society. This [Data Science Ethics Course](https://www.coursera.org/learn/data-science-ethics/home/week/4) describes some of these harms including:

 * **Ossification** - where algorithmic processes have a tendency to "bake in" the current state of the world, creating stereotypes, and making it harder to break out of, or reverse, that mindset.
 * **Distributional Unfairness** - where our experiences are not necessarily reflective of everyone's. Ex: [crowdsourced pothole reporting](https://halpert3.medium.com/six-questions-about-data-science-ethics-252b5ae31fec) favored wealthy neighborhoods that had more cars and phones.
 * **Information Asymmetry** - where [one party has better information than the other](https://en.wikipedia.org/wiki/Information_asymmetry) & creates a power imbalance for decision-making that favors big data! Ex: [How used car dealerships mislead buyers](https://mats.org/the-house-edge-how-dealerships-mislead-their-customers/).
 * **Surveillance** - where proliferation of big data enables predictive policing, with inherent biases creating self-fulfilling feedback loops (see: ossification) that discrimate against specific groups or users.
